{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = '../flipkart_downloaded_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd source_code\n",
    "list_category       = ['full_tshirt', 'full_shirt', 'half_jacket', 'half_shirt', 'half_tshirt', 'tunic_tops', 'half_dress', 'kurta', 'lehenga', 'maxi_dress', 'saree', 'Knee_length_skirt', 'baggy_pant', 'floor_length_skirt', 'formal_pant', 'jeans_pant', 'jeans_short', 'mini_skirt', 'normal_shorts', 'track_pant', 'blouse', 'bomber_jacket', 'coat', 'down_jacket', 'full_cami_tops', 'cami_tops', 'sleeved_crop_tops', 'tank_tops', 'tube_tops', 'half_cami_tops', 'half_tank_tops', 'half_tube_tops', 'hoodie', 'jeans_jacket', 'leather_jacket', 'sleeveless_crop_tops' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ads = glob(f\"{file_location}/*/*.png\")\n",
    "all_ads\n",
    "# total_ads   = [i for i in glob('../dataset/*/*/*.png') if 'original' not in i]\n",
    "# len(total_ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ads   = len([i for i in glob(f\"{file_location}/*/*.png\") if 'original' not in i])\n",
    "try:\n",
    "    with open('logs_ads.txt', 'r') as f:\n",
    "        log = f.readline()\n",
    "except:\n",
    "    log = '0 0 0 0'\n",
    "    print('logs empty')\n",
    "starting = log.split(' ')\n",
    "total_prev = int(starting[-1])\n",
    "print(starting)\n",
    "total_count = 0\n",
    "for cat_num, category in enumerate(list_category):\n",
    "    list_video_ads = glob(f'../flipkart_downloaded_images/{category}/*.png')\n",
    "    category_size = len(list_video_ads)\n",
    "   \n",
    "    for original_count, original_img_path in enumerate(list_video_ads):\n",
    "        original_img   = cv2.resize(cv2.imread(original_img_path), (256, 256))\n",
    "\n",
    "        for ads_count, ads in enumerate(list_video_ads):\n",
    "            \n",
    "            if (original_count > ads_count) or (ads.split('/')[-1].split('_')[0].split(' ')[0] != original_img_path.split('/')[-1].split('_')[0].split(' ')[0]):\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                if total_prev>=total_count:\n",
    "                    total_count += 1\n",
    "                    continue\n",
    "\n",
    "                per = \"{:.2f}\".format((total_count/total_ads)*100)\n",
    "                title = f\"{total_count} / {total_ads}    |  Percentage==>{per} %\"\n",
    "                img_ads = cv2.resize(cv2.imread(ads), (256, 256))\n",
    "\n",
    "                print(ads.split('/')[-1].split('_')[0].split(' ')[0], original_img_path.split('/')[-1].split('_')[0].split(' ')[0])\n",
    "                #print(ads , original_img_path)\n",
    "                convat = cv2.hconcat([original_img, img_ads])\n",
    "                cv2.imshow(title, cv2.resize(convat, (600, 700)))\n",
    "                c = cv2.waitKey(0)\n",
    "                if  c==115: #'s' 'similar'\n",
    "                    with open('dataset_sim_ads.txt', 'a') as f:\n",
    "                        original_path = f\"1|{original_img_path}|{'/'.join(ads.split('/')[1:])}\\n\"\n",
    "                        f.write(original_path)\n",
    "\n",
    "                elif c==100:#'d' dissimilar\n",
    "                    with open('dataset_sim_ads.txt', 'a') as f:\n",
    "                        original_path = f\"0|{original_img_path}|{'/'.join(ads.split('/')[1:])}\\n\"\n",
    "                        f.write(original_path)\n",
    "\n",
    "                elif c==97:\n",
    "                    continue\n",
    "                    \n",
    "                elif c==101:\n",
    "                    break\n",
    "                    \n",
    "                elif c==113:#'q'\n",
    "                    cv2.destroyAllWindows()\n",
    "                    assert(False)\n",
    "                    break\n",
    "\n",
    "                with open('logs_ads.txt', 'w') as f:\n",
    "                    f.write(f\"{cat_num} {original_count} {ads_count} {total_count}\")\n",
    "\n",
    "                total_count += 1\n",
    "                cv2.destroyAllWindows()\n",
    "                \n",
    "       \n",
    "        if total_prev>=total_count:\n",
    "            total_count += 1\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imread('../flipkart_downloaded_images/full_tshirt/gray full_tshirt_906.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if all dataset is avaialble\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt', 'r') as f:\n",
    "    img_paths = [i.replace('\\n', '') for i in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img_path in enumerate(img_paths):\n",
    "    img_path = img_path.split('|')\n",
    "\n",
    "    if os.path.isfile(img_path[1]):\n",
    "        pass\n",
    "    else:\n",
    "        print(i, img_path[1])\n",
    "        #assert(False)\n",
    "        \n",
    "    if os.path.isfile(img_path[2]):\n",
    "        pass\n",
    "    else:\n",
    "        print(i, img_path[2])\n",
    "        #assert(False)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('video-info.csv', 'r') as f:\n",
    "    c1 = f.readlines()\n",
    "with open('videos.csv', 'r') as f:\n",
    "    c2 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43843\n"
     ]
    }
   ],
   "source": [
    "remaining_videos = []\n",
    "for i in c2:\n",
    "    if i in c1:\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        remaining_videos.append(i)\n",
    "        \n",
    "print(len(remaining_videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('remaning_videos.csv', 'w') as f:\n",
    "    f.write(''.join(remaining_videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
